{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a898547",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e15f76c",
   "metadata": {},
   "source": [
    "`(1) Env 환경변수`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f6d77f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0800c924",
   "metadata": {},
   "source": [
    "`(2) 기본 라이브러리`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bca641c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee0237e",
   "metadata": {},
   "source": [
    "## 2. LangChain RAG 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58e5829",
   "metadata": {},
   "source": [
    "### 2.1 문서 로더 (Document Loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64774bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/리비안_KR.txt', 'data/테슬라_KR.txt']\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 파일 목록 가져오기\n",
    "txt_files = glob(os.path.join('data', '*_KR.txt'))\n",
    "\n",
    "print(txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40ccfbbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫 번째 텍스트 파일 내용 확인\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(txt_files[0], encoding='utf-8')\n",
    "data = loader.load()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d67399b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data type 확인\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10b67710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리비안은 MIT 박사 출신 RJ 스카린지가 2009년에 설립한 혁신적인 미국 전기차 제조업체입니다. 2011년부터 자율 전기차에 집중한 리비안은 2015년 대규모 투자를 통해 크게 성장하며 미시간과 베이 지역에 연구소를 설립했습니다. 주요 공급업체와의 접근성을 높이기 위해 본사를 미시간주 리보니아로 이전했습니다.\n",
      "\n",
      "리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)로, 2+2 좌석의 미드 엔진 하이브리드 쿠페로 피터 스티븐스가 디자인했습니다. 이 차는 쉽게 교체 가능한 본체 패널을 갖춘 모듈식 캡슐 구조를 특징으로 하며, 2013년 말에서 2014년 초 사이에 생산이 예상되었습니다. 리비안은 디젤 하이브리드 버전, 브라질 원메이크 시리즈를 위한 R1 GT 레이싱 버전, 4도어 세단 및 크로스오버 등 다양한 버전을 고려했습니다. 2011년에 프로토타입 해치백도 공개되었지만, R1과의 관계는 불명확합니다.\n",
      "\n",
      "리비안은 2021년 10월 첫 번째 양산 차량인 R1T 트럭을 고객에게 인도하기 시작했습니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# page_content를 출력\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e80f6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'data/리비안_KR.txt'}\n"
     ]
    }
   ],
   "source": [
    "# metadata를 출력\n",
    "print(data[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eae2d92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1161.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load all files from a directory\n",
    "from tqdm import tqdm\n",
    "\n",
    "data = []\n",
    "\n",
    "for text_file in tqdm(txt_files):\n",
    "    loader = TextLoader(text_file, encoding='utf-8')\n",
    "    data += loader.load()\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfd2a401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리비안은 MIT 박사 출신 RJ 스카린지가 2009년에 설립한 혁신적인 미국 전기차 제조업체입니다. 2011년부터 자율 전기차에 집중한 리비안은 2015년 대규모 투자를 통해 크게 성장하며 미시간과 베이 지역에 연구소를 설립했습니다. 주요 공급업체와의 접근성을 높이기 위해 본사를 미시간주 리보니아로 이전했습니다.\n",
      "\n",
      "리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)로, 2+2 좌석의 미드 엔진 하이브리드 쿠페로 피터 스티븐스가 디자인했습니다. 이 차는 쉽게 교체 가능한 본체 패널을 갖춘 모듈식 캡슐 구조를 특징으로 하며, 2013년 말에서 2014년 초 사이에 생산이 예상되었습니다. 리비안은 디젤 하이브리드 버전, 브라질 원메이크 시리즈를 위한 R1 GT 레이싱 버전, 4도어 세단 및 크로스오버 등 다양한 버전을 고려했습니다. 2011년에 프로토타입 해치백도 공개되었지만, R1과의 관계는 불명확합니다.\n",
      "\n",
      "리비안은 2021년 10월 첫 번째 양산 차량인 R1T 트럭을 고객에게 인도하기 시작했습니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f1e8c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테슬라(Tesla, Inc.)는 텍사스주 오스틴에 본사를 둔 미국의 대표적인 전기차 제조업체입니다. 2003년 마틴 에버하드(CEO)와 마크 타페닝(CFO)에 의해 설립된 테슬라는 2004년 페이팔과 Zip2의 공동 창업자인 일론 머스크의 참여로 큰 전환점을 맞았습니다. 머스크는 최대 주주이자 회장으로서 회사를 현재의 성공으로 이끌었습니다. 회사 이름은 유명한 물리학자이자 전기공학자인 니콜라 테슬라의 이름을 따서 지어졌습니다. 테슬라는 2010년 6월 나스닥에 상장되었습니다.\n",
      "\n",
      "2023년 테슬라는 1,808,581대의 차량을 판매하여 2022년에 비해 37.65% 증가했습니다. 2012년부터 2023년 3분기까지 테슬라의 전 세계 누적 판매량은 4,962,975대를 초과했습니다. SMT Packaging에 따르면, 2023년 테슬라의 판매량은 전 세계 전기차 시장의 약 12.9%를 차지했습니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a95fa909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9e0da4",
   "metadata": {},
   "source": [
    "### 2.2 텍스트 분할기 (Text Splitters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91af1e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[514, 451]\n"
     ]
    }
   ],
   "source": [
    "# 각 문서의 글자수를 계산\n",
    "char_count = [len(doc.page_content) for doc in data]\n",
    "\n",
    "print(char_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58141c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 282, which is longer than the specified 250\n",
      "Created a chunk of size 268, which is longer than the specified 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 텍스트 청크 수: 5\n",
      "각 청크의 길이: [175, 282, 52, 268, 180]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=250,        # 청크 크기  \n",
    "    chunk_overlap=50,      # 청크 중 중복되는 부분 크기\n",
    "    separator='\\n\\n',      # 청크를 나눌 때 사용할 구분자\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(data)\n",
    "print(f\"생성된 텍스트 청크 수: {len(texts)}\")\n",
    "print(f\"각 청크의 길이: {list(len(text.page_content) for text in texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524ed25",
   "metadata": {},
   "source": [
    "- CharacterTextSplitter는 단순히 문자 수로만 텍스트를 자르지 않습니다. \n",
    "- 기본적으로 문장이나 단락의 경계를 존중하려고 합니다. \n",
    "- 따라서 문장이나 단락의 끝부분에서 지정된 chunk_size를 약간 초과할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c931710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 청크의 내용: 리비안은 MIT 박사 출신 RJ 스카린지가 2009년에 설립한 혁신적인 미국 전기차 제조업체입니다. 2011년부터 자율 전기차에 집중한 리비안은 2015년 대규모 투자를 통해 크게 성장하며 미시간과 베이 지역에 연구소를 설립했습니다. 주요 공급업체와의 접근성을 높이기 위해 본사를 미시간주 리보니아로 이전했습니다.\n"
     ]
    }
   ],
   "source": [
    "print(\"첫 번째 청크의 내용:\", texts[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b21354da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "두 번째 청크의 내용: 리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)로, 2+2 좌석의 미드 엔진 하이브리드 쿠페로 피터 스티븐스가 디자인했습니다. 이 차는 쉽게 교체 가능한 본체 패널을 갖춘 모듈식 캡슐 구조를 특징으로 하며, 2013년 말에서 2014년 초 사이에 생산이 예상되었습니다. 리비안은 디젤 하이브리드 버전, 브라질 원메이크 시리즈를 위한 R1 GT 레이싱 버전, 4도어 세단 및 크로스오버 등 다양한 버전을 고려했습니다. 2011년에 프로토타입 해치백도 공개되었지만, R1과의 관계는 불명확합니다.\n"
     ]
    }
   ],
   "source": [
    "print(\"두 번째 청크의 내용:\", texts[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f44ed1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 청크의 최종 50글자: 했습니다. 주요 공급업체와의 접근성을 높이기 위해 본사를 미시간주 리보니아로 이전했습니다.\n",
      "두 번째 청크의 처음 50글자: 리비안의 초기 모델은 스포츠카 R1(원래 이름은 Avera)로, 2+2 좌석의 미드 엔진 \n"
     ]
    }
   ],
   "source": [
    "# 겹치는 부분을 출력\n",
    "print(\"첫 번째 청크의 최종 50글자:\", texts[0].page_content[-50:])\n",
    "print(\"두 번째 청크의 처음 50글자:\", texts[1].page_content[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab7150f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CharacterTextSplitter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m \u001b[43mCharacterTextSplitter\u001b[49m(\n\u001b[1;32m      2\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m250\u001b[39m,        \u001b[38;5;66;03m# 청크 크기  \u001b[39;00m\n\u001b[1;32m      3\u001b[0m     chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,      \u001b[38;5;66;03m# 청크 중 중복되는 부분 크기\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,          \u001b[38;5;66;03m# 청크를 나눌 때 사용할 구분자\u001b[39;00m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m texts \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(data)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m생성된 텍스트 청크 수: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CharacterTextSplitter' is not defined"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=250,        # 청크 크기  \n",
    "    chunk_overlap=50,      # 청크 중 중복되는 부분 크기\n",
    "    separator='',          # 청크를 나눌 때 사용할 구분자\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(data)\n",
    "print(f\"생성된 텍스트 청크 수: {len(texts)}\")\n",
    "print(f\"각 청크의 길이: {list(len(text.page_content) for text in texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d409d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 청크의 최종 50글자: 이름은 Avera)로, 2+2 좌석의 미드 엔진 하이브리드 쿠페로 피터 스티븐스가 디자인했\n",
      "두 번째 청크의 처음 50글자: 이름은 Avera)로, 2+2 좌석의 미드 엔진 하이브리드 쿠페로 피터 스티븐스가 디자인했\n"
     ]
    }
   ],
   "source": [
    "# 겹치는 부분을 출력\n",
    "print(\"첫 번째 청크의 최종 50글자:\", texts[0].page_content[-50:])\n",
    "print(\"두 번째 청크의 처음 50글자:\", texts[1].page_content[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b010e6",
   "metadata": {},
   "source": [
    "### 2.3 임베딩 모델 (Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f99413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",  # 사용할 모델 이름을 지정 가능 \n",
    ")\n",
    "sample_text = \"테슬라 창업자는 누구인가요?\"\n",
    "vector = embeddings.embed_query(sample_text)\n",
    "print(f\"임베딩 벡터의 차원: {len(vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8825e64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 벡터의 차원: 768\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# API 키 설정\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# 텍스트 임베딩 생성\n",
    "sample_text = \"테슬라 창업자는 누구인가요?\"\n",
    "result = genai.embed_content(\n",
    "    model=\"models/text-embedding-004\",  # 사용할 모델 이름\n",
    "    content=sample_text                # 임베딩할 텍스트\n",
    ")\n",
    "\n",
    "# 임베딩 결과\n",
    "vector = result['embedding']\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"임베딩 벡터의 차원: {len(vector)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a797da44",
   "metadata": {},
   "source": [
    "### 2.4 벡터 저장소 (Vector Stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "21592998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'data/리비안_KR.txt'}, page_content='리비안은 MIT 박사 출신 RJ 스카린지가 2009년에 설립한 혁신적인 미국 전기차 제조업체입니다. 2011년부터 자율 전기차에 집중한 리비안은 2015년 대규모 투자를 통해 크게 성장하며 미시간과 베이 지역에 연구소를 설립했습니다. 주요 공급업체와의 접근성을 높이기 위해 본사를 미시간주 리보니아로 이전했습니다.')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "38bf8227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벡터 저장소에 저장된 문서 수: 5\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=texts, \n",
    "    embedding=embeddings,\n",
    "    collection_name=\"chroma_test\", \n",
    "    persist_directory=\"./chroma_db\",\n",
    "    )\n",
    "print(f\"벡터 저장소에 저장된 문서 수: {vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30c77707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벡터 저장소에 저장된 문서 수: 10\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "class GeminiEmbeddings:\n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"문서 리스트를 임베딩으로 변환\"\"\"\n",
    "        return [\n",
    "            genai.embed_content(\n",
    "                model=\"models/text-embedding-004\",\n",
    "                content=text\n",
    "            )['embedding']\n",
    "            for text in texts\n",
    "        ]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        \"\"\"단일 텍스트 쿼리를 임베딩으로 변환\"\"\"\n",
    "        result = genai.embed_content(\n",
    "            model=\"models/text-embedding-004\",\n",
    "            content=text\n",
    "        )\n",
    "        return result['embedding']\n",
    "\n",
    "# 커스텀 임베딩 클래스 초기화\n",
    "embeddings = GeminiEmbeddings()\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=texts, \n",
    "    embedding=embeddings,\n",
    "    collection_name=\"chroma_test\", \n",
    "    persist_directory=\"./chroma_db\",\n",
    "    )\n",
    "print(f\"벡터 저장소에 저장된 문서 수: {vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d2005fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색 결과의 길이: 4\n",
      "검색 결과의 첫 번째 문서: 리비안은 2021년 10월 첫 번째 양산 차량인 R1T 트럭을 고객에게 인도하기 시작했습니다....\n"
     ]
    }
   ],
   "source": [
    "# 문서 검색\n",
    "query = \"테슬라 창업자는 누구인가요?\"\n",
    "result = vectorstore.similarity_search(query)\n",
    "\n",
    "print(f\"검색 결과의 길이: {len(result)}\")\n",
    "print(f\"검색 결과의 첫 번째 문서: {result[0].page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ebf78a",
   "metadata": {},
   "source": [
    "### 2.5 검색기 (Retrievers)\n",
    "   - 질의에 관련된 문서를 검색하는 컴포넌트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff565fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색된 관련 문서 수: 2\n",
      "첫 번째 관련 문서 내용 미리보기: 리비안은 2021년 10월 첫 번째 양산 차량인 R1T 트럭을 고객에게 인도하기 시작했습니다....\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "relevant_docs = retriever.invoke(query)\n",
    "print(f\"검색된 관련 문서 수: {len(relevant_docs)}\")\n",
    "print(f\"첫 번째 관련 문서 내용 미리보기: {relevant_docs[0].page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2cc440",
   "metadata": {},
   "source": [
    "### 2.6 언어 모델 (LLMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c242db85",
   "metadata": {},
   "outputs": [
    {
     "ename": "PydanticUserError",
     "evalue": "The `__modify_schema__` method is not supported in Pydantic v2. Use `__get_pydantic_json_schema__` instead in class `SecretStr`.\n\nFor further information visit https://errors.pydantic.dev/2.10/u/custom-json-schema",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPydanticUserError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Context를 제공하지 않고 답변 생성 (Hallucination 발생 위험)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m      5\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOpenAI(\n\u001b[1;32m      6\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# 사용할 모델 이름을 지정 가능\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,        \u001b[38;5;66;03m# 답변의 창의성을 조절\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,       \u001b[38;5;66;03m# 생성할 최대 토큰 수\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     11\u001b[0m response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m테슬라 창업자는 누구인가요?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/langchain_openai/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI, ChatOpenAI\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAIEmbeddings, OpenAIEmbeddings\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI, OpenAI\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/langchain_openai/chat_models/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAzureChatOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/langchain_openai/chat_models/azure.py:41\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction_calling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_to_openai_tool\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_basemodel_subclass\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseChatOpenAI\n\u001b[1;32m     43\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     46\u001b[0m _BM \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_BM\u001b[39m\u001b[38;5;124m\"\u001b[39m, bound\u001b[38;5;241m=\u001b[39mBaseModel)\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:353\u001b[0m\n\u001b[1;32m    349\u001b[0m     parsed: Optional[_DictOrPydantic]\n\u001b[1;32m    350\u001b[0m     parsing_error: Optional[\u001b[38;5;167;01mBaseException\u001b[39;00m]\n\u001b[0;32m--> 353\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43;01mBaseChatOpenAI\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBaseChatModel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mAny\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mField\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#: :meta private:\u001b[39;49;00m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_client\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mAny\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mField\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#: :meta private:\u001b[39;49;00m\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/pydantic/_internal/_model_construction.py:224\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[0;34m(mcs, cls_name, bases, namespace, __pydantic_generic_metadata__, __pydantic_reset_parent_namespace__, _create_model_module, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_wrapper\u001b[38;5;241m.\u001b[39mfrozen \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__hash__\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m namespace:\n\u001b[1;32m    222\u001b[0m     set_default_hash_func(\u001b[38;5;28mcls\u001b[39m, bases)\n\u001b[0;32m--> 224\u001b[0m \u001b[43mcomplete_model_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcls_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraise_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mns_resolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mns_resolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_model_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_create_model_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# If this is placed before the complete_model_class call above,\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# the generic computed fields return type is set to PydanticUndefined\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_computed_fields__ \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    236\u001b[0m     k: v\u001b[38;5;241m.\u001b[39minfo \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_decorators__\u001b[38;5;241m.\u001b[39mcomputed_fields\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    237\u001b[0m }\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/pydantic/_internal/_model_construction.py:602\u001b[0m, in \u001b[0;36mcomplete_model_class\u001b[0;34m(cls, cls_name, config_wrapper, raise_errors, ns_resolver, create_model_module)\u001b[0m\n\u001b[1;32m    595\u001b[0m handler \u001b[38;5;241m=\u001b[39m CallbackGetCoreSchemaHandler(\n\u001b[1;32m    596\u001b[0m     partial(gen_schema\u001b[38;5;241m.\u001b[39mgenerate_schema, from_dunder_get_core_schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m    597\u001b[0m     gen_schema,\n\u001b[1;32m    598\u001b[0m     ref_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munpack\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    599\u001b[0m )\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 602\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_pydantic_core_schema__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PydanticUndefinedAnnotation \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raise_errors:\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/pydantic/main.py:702\u001b[0m, in \u001b[0;36mBaseModel.__get_pydantic_core_schema__\u001b[0;34m(cls, source, handler)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_generic_metadata__[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morigin\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_core_schema__\n\u001b[0;32m--> 702\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/pydantic/_internal/_schema_generation_shared.py:84\u001b[0m, in \u001b[0;36mCallbackGetCoreSchemaHandler.__call__\u001b[0;34m(self, source_type)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source_type: Any, \u001b[38;5;241m/\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mCoreSchema:\n\u001b[0;32m---> 84\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     ref \u001b[38;5;241m=\u001b[39m schema\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ref_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto-def\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:610\u001b[0m, in \u001b[0;36mGenerateSchema.generate_schema\u001b[0;34m(self, obj, from_dunder_get_core_schema)\u001b[0m\n\u001b[1;32m    607\u001b[0m         schema \u001b[38;5;241m=\u001b[39m from_property\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_schema_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    612\u001b[0m metadata_js_function \u001b[38;5;241m=\u001b[39m _extract_get_pydantic_json_schema(obj, schema)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadata_js_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:879\u001b[0m, in \u001b[0;36mGenerateSchema._generate_schema_inner\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lenient_issubclass(obj, BaseModel):\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type_stack\u001b[38;5;241m.\u001b[39mpush(obj):\n\u001b[0;32m--> 879\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, PydanticRecursiveRef):\n\u001b[1;32m    882\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema\u001b[38;5;241m.\u001b[39mdefinition_reference_schema(schema_ref\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mtype_ref)\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:691\u001b[0m, in \u001b[0;36mGenerateSchema._model_schema\u001b[0;34m(self, cls)\u001b[0m\n\u001b[1;32m    679\u001b[0m     model_schema \u001b[38;5;241m=\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mmodel_schema(\n\u001b[1;32m    680\u001b[0m         \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m    681\u001b[0m         inner_schema,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    687\u001b[0m         ref\u001b[38;5;241m=\u001b[39mmodel_ref,\n\u001b[1;32m    688\u001b[0m     )\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    690\u001b[0m     fields_schema: core_schema\u001b[38;5;241m.\u001b[39mCoreSchema \u001b[38;5;241m=\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mmodel_fields_schema(\n\u001b[0;32m--> 691\u001b[0m         \u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_md_field_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecorators\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m,\n\u001b[1;32m    692\u001b[0m         computed_fields\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    693\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed_field_schema(d, decorators\u001b[38;5;241m.\u001b[39mfield_serializers)\n\u001b[1;32m    694\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m computed_fields\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    695\u001b[0m         ],\n\u001b[1;32m    696\u001b[0m         extras_schema\u001b[38;5;241m=\u001b[39mextras_schema,\n\u001b[1;32m    697\u001b[0m         model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    698\u001b[0m     )\n\u001b[1;32m    699\u001b[0m     inner_schema \u001b[38;5;241m=\u001b[39m apply_validators(fields_schema, decorators\u001b[38;5;241m.\u001b[39mroot_validators\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    700\u001b[0m     new_inner_schema \u001b[38;5;241m=\u001b[39m define_expected_missing_refs(inner_schema, recursively_defined_type_refs())\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:691\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    679\u001b[0m     model_schema \u001b[38;5;241m=\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mmodel_schema(\n\u001b[1;32m    680\u001b[0m         \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m    681\u001b[0m         inner_schema,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    687\u001b[0m         ref\u001b[38;5;241m=\u001b[39mmodel_ref,\n\u001b[1;32m    688\u001b[0m     )\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    690\u001b[0m     fields_schema: core_schema\u001b[38;5;241m.\u001b[39mCoreSchema \u001b[38;5;241m=\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mmodel_fields_schema(\n\u001b[0;32m--> 691\u001b[0m         {k: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_md_field_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecorators\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m fields\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[1;32m    692\u001b[0m         computed_fields\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    693\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_computed_field_schema(d, decorators\u001b[38;5;241m.\u001b[39mfield_serializers)\n\u001b[1;32m    694\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m computed_fields\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    695\u001b[0m         ],\n\u001b[1;32m    696\u001b[0m         extras_schema\u001b[38;5;241m=\u001b[39mextras_schema,\n\u001b[1;32m    697\u001b[0m         model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    698\u001b[0m     )\n\u001b[1;32m    699\u001b[0m     inner_schema \u001b[38;5;241m=\u001b[39m apply_validators(fields_schema, decorators\u001b[38;5;241m.\u001b[39mroot_validators\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    700\u001b[0m     new_inner_schema \u001b[38;5;241m=\u001b[39m define_expected_missing_refs(inner_schema, recursively_defined_type_refs())\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:1071\u001b[0m, in \u001b[0;36mGenerateSchema._generate_md_field_schema\u001b[0;34m(self, name, field_info, decorators)\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate_md_field_schema\u001b[39m(\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1066\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   1067\u001b[0m     field_info: FieldInfo,\n\u001b[1;32m   1068\u001b[0m     decorators: DecoratorInfos,\n\u001b[1;32m   1069\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mModelField:\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prepare a ModelField to represent a model field.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1071\u001b[0m     common_field \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_common_field_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecorators\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema\u001b[38;5;241m.\u001b[39mmodel_field(\n\u001b[1;32m   1073\u001b[0m         common_field[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mschema\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   1074\u001b[0m         serialization_exclude\u001b[38;5;241m=\u001b[39mcommon_field[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserialization_exclude\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1078\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mcommon_field[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   1079\u001b[0m     )\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:1263\u001b[0m, in \u001b[0;36mGenerateSchema._common_field_schema\u001b[0;34m(self, name, field_info, decorators)\u001b[0m\n\u001b[1;32m   1259\u001b[0m         schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_annotations(\n\u001b[1;32m   1260\u001b[0m             source_type, annotations \u001b[38;5;241m+\u001b[39m validators_from_decorators, transform_inner_schema\u001b[38;5;241m=\u001b[39mset_discriminator\n\u001b[1;32m   1261\u001b[0m         )\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1263\u001b[0m         schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_annotations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m            \u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m            \u001b[49m\u001b[43mannotations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidators_from_decorators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;66;03m# This V1 compatibility shim should eventually be removed\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;66;03m# push down any `each_item=True` validators\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;66;03m# note that this won't work for any Annotated types that get wrapped by a function validator\u001b[39;00m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;66;03m# but that's okay because that didn't exist in V1\u001b[39;00m\n\u001b[1;32m   1272\u001b[0m this_field_validators \u001b[38;5;241m=\u001b[39m filter_field_decorator_info_by_field(decorators\u001b[38;5;241m.\u001b[39mvalidators\u001b[38;5;241m.\u001b[39mvalues(), name)\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2056\u001b[0m, in \u001b[0;36mGenerateSchema._apply_annotations\u001b[0;34m(self, source_type, annotations, transform_inner_schema)\u001b[0m\n\u001b[1;32m   2051\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   2052\u001b[0m     get_inner_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_wrapped_inner_schema(\n\u001b[1;32m   2053\u001b[0m         get_inner_schema, annotation, pydantic_js_annotation_functions\n\u001b[1;32m   2054\u001b[0m     )\n\u001b[0;32m-> 2056\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mget_inner_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pydantic_js_annotation_functions:\n\u001b[1;32m   2058\u001b[0m     core_metadata \u001b[38;5;241m=\u001b[39m schema\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m, {})\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/pydantic/_internal/_schema_generation_shared.py:84\u001b[0m, in \u001b[0;36mCallbackGetCoreSchemaHandler.__call__\u001b[0;34m(self, source_type)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source_type: Any, \u001b[38;5;241m/\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m core_schema\u001b[38;5;241m.\u001b[39mCoreSchema:\n\u001b[0;32m---> 84\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     ref \u001b[38;5;241m=\u001b[39m schema\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ref_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto-def\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2037\u001b[0m, in \u001b[0;36mGenerateSchema._apply_annotations.<locals>.inner_handler\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2035\u001b[0m from_property \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_schema_from_property(obj, source_type)\n\u001b[1;32m   2036\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_property \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2037\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_schema_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2038\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2039\u001b[0m     schema \u001b[38;5;241m=\u001b[39m from_property\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:884\u001b[0m, in \u001b[0;36mGenerateSchema._generate_schema_inner\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, PydanticRecursiveRef):\n\u001b[1;32m    882\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema\u001b[38;5;241m.\u001b[39mdefinition_reference_schema(schema_ref\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mtype_ref)\n\u001b[0;32m--> 884\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:986\u001b[0m, in \u001b[0;36mGenerateSchema.match_type\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    984\u001b[0m origin \u001b[38;5;241m=\u001b[39m get_origin(obj)\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 986\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_match_generic_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_prepare_pydantic_annotations_for_known_type(obj, ())\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:1014\u001b[0m, in \u001b[0;36mGenerateSchema._match_generic_type\u001b[0;34m(self, obj, origin)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_type_alias_type_schema(obj)\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _typing_extra\u001b[38;5;241m.\u001b[39morigin_is_union(origin):\n\u001b[0;32m-> 1014\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_union_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m origin \u001b[38;5;129;01min\u001b[39;00m TUPLE_TYPES:\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tuple_schema(obj)\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:1325\u001b[0m, in \u001b[0;36mGenerateSchema._union_schema\u001b[0;34m(self, union_type)\u001b[0m\n\u001b[1;32m   1323\u001b[0m         nullable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1325\u001b[0m         choices\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(choices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1328\u001b[0m     s \u001b[38;5;241m=\u001b[39m choices[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:612\u001b[0m, in \u001b[0;36mGenerateSchema.generate_schema\u001b[0;34m(self, obj, from_dunder_get_core_schema)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    610\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_schema_inner(obj)\n\u001b[0;32m--> 612\u001b[0m metadata_js_function \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_get_pydantic_json_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadata_js_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    614\u001b[0m     metadata_schema \u001b[38;5;241m=\u001b[39m resolve_original_schema(schema, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefs\u001b[38;5;241m.\u001b[39mdefinitions)\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2395\u001b[0m, in \u001b[0;36m_extract_get_pydantic_json_schema\u001b[0;34m(tp, schema)\u001b[0m\n\u001b[1;32m   2393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_custom_v2_modify_js_func:\n\u001b[1;32m   2394\u001b[0m         cls_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(tp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 2395\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[1;32m   2396\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe `__modify_schema__` method is not supported in Pydantic v2. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2397\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUse `__get_pydantic_json_schema__` instead\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in class `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mcls_name\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   2398\u001b[0m             code\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom-json-schema\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   2399\u001b[0m         )\n\u001b[1;32m   2401\u001b[0m \u001b[38;5;66;03m# handle GenericAlias' but ignore Annotated which \"lies\" about its origin (in this case it would be `int`)\u001b[39;00m\n\u001b[1;32m   2402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__origin__\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _typing_extra\u001b[38;5;241m.\u001b[39mis_annotated(tp):\n",
      "\u001b[0;31mPydanticUserError\u001b[0m: The `__modify_schema__` method is not supported in Pydantic v2. Use `__get_pydantic_json_schema__` instead in class `SecretStr`.\n\nFor further information visit https://errors.pydantic.dev/2.10/u/custom-json-schema"
     ]
    }
   ],
   "source": [
    "# Context를 제공하지 않고 답변 생성 (Hallucination 발생 위험)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # 사용할 모델 이름을 지정 가능\n",
    "    temperature=0,        # 답변의 창의성을 조절\n",
    "    max_tokens=100,       # 생성할 최대 토큰 수\n",
    "    )\n",
    "\n",
    "response = llm.invoke(\"테슬라 창업자는 누구인가요?\")\n",
    "print(f\"LLM 응답: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "851bd4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM 응답: content=' 네, 퍼플아이오를 알고 있습니다. 퍼플아이오는 인공지능(AI) 기반의 이미지 생성 서비스입니다. 사용자가 원하는 이미지를 텍스트로 입력하면, AI가 그 이미지를 생성해줍니다. 퍼플아이오는 다양한 스타일과 해상도의 이미지를 생성할 수 있으며, 사용자는 생성된 이미지를 다운로드하거나 공유할 수 있습니다.\\n\\n퍼플아이' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'MAX_TOKENS', 'safety_ratings': []} id='run-daa327c6-3528-4c87-bc23-f7fbf3043b67-0' usage_metadata={'input_tokens': 8, 'output_tokens': 100, 'total_tokens': 108, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "# import google.generativeai as genai\n",
    "# import os\n",
    "\n",
    "# genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# from google.api_core import retry\n",
    "\n",
    "# @retry.Retry()\n",
    "# def generate_text(*args, **kwargs):\n",
    "#   return genai.generate_text(*args, **kwargs)\n",
    "\n",
    "# models = [m for m in genai.list_models() if 'generateText' in m.supported_generation_methods]\n",
    "# model = models[0].name\n",
    "# print(model)\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "# 다중 메시지 전송\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=100,\n",
    "    google_api_key=os.environ[\"GEMINI_API_KEY\"]\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"퍼플아이오 알아?\")\n",
    "print(f\"LLM 응답: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96429ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFound",
     "evalue": "404 Requested entity was not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m\n\u001b[1;32m      1\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mI have 77 houses, each with 31 cats.\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mEach cat owns 14 mittens, and 6 hats.\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124mEach mitten was knit from 141m of yarn, each hat from 55m.\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124mHow much yarn was needed to make all the items?\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      8\u001b[0m prompt_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124mYou are an expert at solving word problems. Here\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms one:\u001b[39m\n\u001b[1;32m     10\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124mYour solution:\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 19\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# The maximum length of the response\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(completion\u001b[38;5;241m.\u001b[39mresult)\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;129m@retry\u001b[39m\u001b[38;5;241m.\u001b[39mRetry()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_text\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 10\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/google/generativeai/text.py:205\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(model, prompt, temperature, candidate_count, max_output_tokens, top_p, top_k, safety_settings, stop_sequences, client, request_options)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the API to generate text based on the provided prompt.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03m    A `types.Completion` containing the model's text completion response.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    193\u001b[0m request \u001b[38;5;241m=\u001b[39m _make_generate_text_request(\n\u001b[1;32m    194\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    195\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m     stop_sequences\u001b[38;5;241m=\u001b[39mstop_sequences,\n\u001b[1;32m    203\u001b[0m )\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_generate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/google/generativeai/text.py:243\u001b[0m, in \u001b[0;36m_generate_response\u001b[0;34m(request, client, request_options)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     client \u001b[38;5;241m=\u001b[39m get_default_text_client()\n\u001b[0;32m--> 243\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(response)\u001b[38;5;241m.\u001b[39mto_dict(response)\n\u001b[1;32m    246\u001b[0m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilters\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m palm_safety_types\u001b[38;5;241m.\u001b[39mconvert_filters_to_enums(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilters\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/text_service/client.py:880\u001b[0m, in \u001b[0;36mTextServiceClient.generate_text\u001b[0;34m(self, request, model, prompt, temperature, candidate_count, max_output_tokens, top_p, top_k, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    879\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 880\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/google/api_core/timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mNotFound\u001b[0m: 404 Requested entity was not found."
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "I have 77 houses, each with 31 cats.\n",
    "Each cat owns 14 mittens, and 6 hats.\n",
    "Each mitten was knit from 141m of yarn, each hat from 55m.\n",
    "How much yarn was needed to make all the items?\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are an expert at solving word problems. Here's one:\n",
    "\n",
    "{question}\n",
    "\n",
    "Work through it step by step, and show your work.\n",
    "One step per line.\n",
    "\n",
    "Your solution:\n",
    "\"\"\"\n",
    "\n",
    "completion = generate_text(\n",
    "    model=model,\n",
    "    prompt=prompt_template.format(question=question),\n",
    "    # The maximum length of the response\n",
    "    max_output_tokens=800,\n",
    ")\n",
    "\n",
    "final_prompt = prompt_template.format(question=question)\n",
    "completion = generate_text(final_prompt)\n",
    "print(completion)\n",
    "\n",
    "print(completion.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1af2c464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM 응답: 테슬라 창업자는 일론 머스크입니다.  하지만 테슬라는 일론 머스크가 혼자서 만든 회사는 아니며, **Martin Eberhard**, **Marc Tarpenning**, **Ian Wright**, **JB Straubel**과 함께 2003년에 공동 창업했습니다.  일론 머스크는 초기 투자자로 참여했고, 나중에 회사의 CEO가 되었습니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from google.api_core import retry\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "@retry.Retry()\n",
    "def generate_text(prompt):\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "# Gemini AI Chat 클래스 정의\n",
    "class ChatGeminiAI:\n",
    "    def __init__(self, model, temperature=0, max_tokens=100):\n",
    "        self.model = model = genai.GenerativeModel(model)\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        genai.GenerationConfig(temperature=temperature, max_output_tokens=max_tokens)\n",
    "\n",
    "    def invoke(self, prompt):\n",
    "        response = generate_text(prompt)\n",
    "        return response\n",
    "\n",
    "#genai.GenerationConfig(temperature=0, max_output_tokens=100)\n",
    "# LLM 객체 생성\n",
    "llm = ChatGeminiAI(\n",
    "    model='gemini-1.5-flash',  # 사용할 모델 이름 지정\n",
    "    temperature=0,                 # 답변의 창의성을 조절\n",
    "    max_tokens=100                 # 생성할 최대 토큰 수\n",
    ")\n",
    "\n",
    "# 질문에 대한 응답 생성\n",
    "response = llm.invoke(\"테슬라 창업자는 누구인가요?\")\n",
    "print(f\"LLM 응답: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b702f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Calculate the total number of cats: 77 houses * 31 cats/house = 2387 cats\n",
      "\n",
      "Step 2: Calculate the total number of mittens: 2387 cats * 14 mittens/cat = 33418 mittens\n",
      "\n",
      "Step 3: Calculate the total meters of yarn for mittens: 33418 mittens * 141 m/mitten = 4713158 meters\n",
      "\n",
      "Step 4: Calculate the total number of hats: 2387 cats * 6 hats/cat = 14322 hats\n",
      "\n",
      "Step 5: Calculate the total meters of yarn for hats: 14322 hats * 55 m/hat = 787710 meters\n",
      "\n",
      "Step 6: Calculate the total meters of yarn needed: 4713158 meters + 787710 meters = 5500868 meters\n",
      "\n",
      "Therefore, a total of 5,500,868 meters of yarn was needed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from google.api_core import retry\n",
    "\n",
    "# API 키 설정\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# 모델 설정 - text-only 모델 사용\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "genai.GenerationConfig(temperature=0, max_output_tokens=100)\n",
    "\n",
    "@retry.Retry()\n",
    "def generate_text(prompt):\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "# 문제 정의\n",
    "question = \"\"\"\n",
    "I have 77 houses, each with 31 cats.\n",
    "Each cat owns 14 mittens, and 6 hats.\n",
    "Each mitten was knit from 141m of yarn, each hat from 55m.\n",
    "How much yarn was needed to make all the items?\n",
    "\"\"\"\n",
    "\n",
    "# 프롬프트 템플릿\n",
    "prompt_template = \"\"\"\n",
    "You are an expert at solving word problems. Here's one:\n",
    "\n",
    "{question}\n",
    "\n",
    "Work through it step by step, and show your work.\n",
    "One step per line.\n",
    "\n",
    "Your solution:\n",
    "\"\"\"\n",
    "\n",
    "# 텍스트 생성\n",
    "final_prompt = prompt_template.format(question=question)\n",
    "completion = generate_text(prompt=final_prompt)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7e372959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리비안은 2021년 10월 첫 번째 양산 차량인 R1T 트럭을 고객에게 인도하기 시작했습니다.\n",
      "\n",
      "위 내용에 근거하여 다음 질문에 답변하세요. \n",
      "\n",
      "테슬라 창업자는 누구인가요?\n"
     ]
    }
   ],
   "source": [
    "# Context를 제공하고 답변 생성\n",
    "\n",
    "query_with_context = f\"\"\"{relevant_docs[0].page_content}\\n\\n위 내용에 근거하여 다음 질문에 답변하세요. \\n\\n{query}\"\"\"\n",
    "print(query_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ac0516f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM 응답: 질문에 대한 답은 제공된 텍스트에는 없습니다.  텍스트는 리비안의 양산 시작일을 언급하고 있을 뿐, 테슬라 창업자에 대한 정보는 포함하고 있지 않습니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(query_with_context)\n",
    "print(f\"LLM 응답: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d7177b",
   "metadata": {},
   "source": [
    "### 2.7 전체 RAG 파이프라인 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d12f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "\n",
    "\n",
    "# LLM 초기화\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 프롬프트 - 옵션 1 (허브 사용)\n",
    "# https://smith.langchain.com/hub/langchain-ai/retrieval-qa-chat\n",
    "# retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "\n",
    "# 프롬프트 - 옵션 2 (직접 작성)\n",
    "retrieval_qa_chat_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "다음 컨텍스트를 바탕으로 질문에 답변해주세요. 컨텍스트에 관련 정보가 없다면, \n",
    "\"주어진 정보로는 답변할 수 없습니다.\"라고 말씀해 주세요.\n",
    "\n",
    "컨텍스트: {context}\n",
    "\n",
    "질문: {input}\n",
    "\n",
    "답변:\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# 체인 생성\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, retrieval_qa_chat_prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "# 체인 실행\n",
    "query = \"테슬라 창업자는 누구인가요?\"\n",
    "response = rag_chain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "652017b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",  # 사용할 모델 이름을 지정 가능\n",
    "    temperature=0,                 # 답변의 창의성을 조절\n",
    "    max_tokens=100,                 # 생성할 최대 토큰 수\n",
    "    google_api_key=os.environ[\"GEMINI_API_KEY\"]\n",
    ")\n",
    "\n",
    "retrieval_qa_chat_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "다음 컨텍스트를 바탕으로 질문에 답변해주세요. 컨텍스트에 관련 정보가 없다면, \n",
    "\"주어진 정보로는 답변할 수 없습니다.\"라고 말씀해 주세요.\n",
    "\n",
    "컨텍스트: {context}\n",
    "\n",
    "질문: {input}\n",
    "\n",
    "답변:\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# 체인 생성\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, retrieval_qa_chat_prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "\n",
    "# 체인 실행\n",
    "query = \"테슬라 창업자는 누구인가요?\"\n",
    "response = rag_chain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2b7fc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '테슬라 창업자는 누구인가요?', 'context': [Document(metadata={'source': 'data/리비안_KR.txt'}, page_content='리비안은 2021년 10월 첫 번째 양산 차량인 R1T 트럭을 고객에게 인도하기 시작했습니다.'), Document(metadata={'source': 'data/리비안_KR.txt'}, page_content='리비안은 2021년 10월 첫 번째 양산 차량인 R1T 트럭을 고객에게 인도하기 시작했습니다.')], 'answer': '주어진 정보로는 답변할 수 없습니다.'}\n"
     ]
    }
   ],
   "source": [
    "# 결과 출력\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30d73253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input', 'context', 'answer'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 응답 객체의 키 확인\n",
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db0becb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'주어진 정보로는 답변할 수 없습니다.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f30cec6",
   "metadata": {},
   "source": [
    "`[추가 설명] create_stuff_documents_chain`\n",
    "- 검색된 문서들을 하나의 컨텍스트로 결합하고, 이를 바탕으로 질문에 답변하는 체인을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25e346b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain은 파이썬과 자바스크립트를 지원합니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "stuff_chain = create_stuff_documents_chain(llm, retrieval_qa_chat_prompt)\n",
    "\n",
    "documents = [\n",
    "    Document(page_content=\"LangChain은 LLM 애플리케이션 개발을 위한 프레임워크입니다.\"),\n",
    "    Document(page_content=\"LangChain은 파이썬과 자바스크립트를 지원합니다.\")\n",
    "]\n",
    "response = stuff_chain.invoke({\n",
    "    \"input\": \"LangChain은 어떤 언어를 지원하나요?\",\n",
    "    \"context\": documents\n",
    "})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bf401f",
   "metadata": {},
   "source": [
    "`[추가 설명] create_retrieval_chain`\n",
    "- 질문에 관련된 문서를 검색하고, 검색된 문서를 바탕으로 답변을 생성하는 전체 RAG 파이프라인을 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f85d8b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '테슬라 창업자는 누구인가요?', 'context': [Document(metadata={'source': 'data/리비안_KR.txt'}, page_content='리비안은 2021년 10월 첫 번째 양산 차량인 R1T 트럭을 고객에게 인도하기 시작했습니다.'), Document(metadata={'source': 'data/리비안_KR.txt'}, page_content='리비안은 2021년 10월 첫 번째 양산 차량인 R1T 트럭을 고객에게 인도하기 시작했습니다.'), Document(metadata={'source': 'data/리비안_KR.txt'}, page_content='리비안은 MIT 박사 출신 RJ 스카린지가 2009년에 설립한 혁신적인 미국 전기차 제조업체입니다. 2011년부터 자율 전기차에 집중한 리비안은 2015년 대규모 투자를 통해 크게 성장하며 미시간과 베이 지역에 연구소를 설립했습니다. 주요 공급업체와의 접근성을 높이기 위해 본사를 미시간주 리보니아로 이전했습니다.'), Document(metadata={'source': 'data/리비안_KR.txt'}, page_content='리비안은 MIT 박사 출신 RJ 스카린지가 2009년에 설립한 혁신적인 미국 전기차 제조업체입니다. 2011년부터 자율 전기차에 집중한 리비안은 2015년 대규모 투자를 통해 크게 성장하며 미시간과 베이 지역에 연구소를 설립했습니다. 주요 공급업체와의 접근성을 높이기 위해 본사를 미시간주 리보니아로 이전했습니다.')], 'answer': '주어진 정보로는 답변할 수 없습니다.'}\n"
     ]
    }
   ],
   "source": [
    "# 검색 체인 생성\n",
    "retrieval_chain = create_retrieval_chain(vectorstore.as_retriever(), stuff_chain)\n",
    "\n",
    "# 체인 실행\n",
    "query = \"테슬라 창업자는 누구인가요?\"\n",
    "response = retrieval_chain.invoke({\"input\": query})\n",
    "\n",
    "# 결과 출력\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceb9f42",
   "metadata": {},
   "source": [
    "## 3. Gradio 챗봇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b207180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeongyeong-yeon/gy_jeon/rag-master-study/myenv/lib/python3.11/site-packages/gradio/analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.44.0, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def answer_invoke(message, history):\n",
    "    response = rag_chain.invoke({\"input\": message})\n",
    "    return response[\"answer\"]\n",
    "\n",
    "\n",
    "# Graiio 인터페이스 생성 \n",
    "demo = gr.ChatInterface(fn=answer_invoke, title=\"QA Bot\")\n",
    "\n",
    "# Graiio 실행  \n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "82f7a6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "# Graiio 종료\n",
    "demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
